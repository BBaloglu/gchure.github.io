---
layout: post
title: "Scientific Showcase"
categories: none
sect: home 
image: random_walk_expon.png
---

In addition to photography, I focus my artistic streak on making high-quality
illustrations for talks, papers, and presentations. I also think that the future
of publishing will **not** include `PDF` file formats. Therefore, I enjoy making
interactive figures and pedagogical tools. This page shows a bunch of those
examples with some descriptions.


## Interactive Science

### The Bayesian coin flipper
Coin flips are a ubiquitous pedagogical tool for teaching the principles behind
stochastic processes and probability theory. In both contexts, we are often
interested in determining how "biased" the coin is. To determine this bias, we
can use [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) to assign a *probability* to a given value for the
bias. Let's say that we have $$N$$ coin flips, $n$ of which landed heads-up.
Bayes' theorem tells us that the probability of the coins' bias *towards* head
is proportional to the likelihood of observing $$n$$ heads given $$N$$ flips and
a bias of $$p$$ multiplied by our prior knowledge of the possible values of $$p$$ 
knowing *nothing* about the flip outcomes. We can state this
mathematically as 

$$
g(p\,\vert\, N, n) \propto f(n\,\vert\, N, p)\,g(p),
$$

where $$g$$ and $$f$$ correspond to probability densities over parameters and data,
respectively. Without going into too much detail, the number of heads we observe
given $$N$$ flips should be Binomially distributed with a probability $$p$$,

$$
f(n\,\vert\,N, p) = \frac{N!}{n! (N - n)!}p^n(1 - p)^{N - n}.
$$

Choosing a functional form for the prior distribution $$g(p)$$ is a subjective
and tricky one. Knowing *nothing* about the coin, we could say that the
probability $$p$$ could be anywhere between 0 and 1. Another option would be to
provide a highly informative prior about some value, say 0.5 if we trust the
coin. An example of such a highly informative prior would be a Gaussian
distribution,

$$
g(p) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{p - \mu}{\sigma^2}},
$$

where $$\mu$$ and $$\sigma$$ correspond to the mean and standard deviation.
Of course, probabilities outside the bounds of 0 and 1 are impossible, so a
Gaussian prior isn't completely appropriate. For pedagogical reasons, however,
we can simply say that any value outside these bounds must be zero.

In the tool below, you can see the effect the choice of prior parameters, 
coin bias, and number of coin flips influences the posterior probability 
distribution. To use this tool, click the button to generate $$10^6$$ coin flips
with a given coin bias. By sliding the number of displayed flips and prior
parameters, you can see how the prior distribution (orange) influences the shape
of the posterior distribution (purple). One can approximate a uniform prior by
making the $$\sigma$$ of the prior very large.

<center>
{% include bayesian_coin_flipper.html %}
</center>

This tool was built using the [Bokeh plotting library](http://bokeh.pydata.org)
in Python. The script used to generate it can be downloaded here:
[`bayesian_coin_flipper.py`]({{site.baseurl}}/assets/code/bayesian_coin_flipper.py).
